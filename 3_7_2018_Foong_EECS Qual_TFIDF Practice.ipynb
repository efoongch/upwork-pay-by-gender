{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_writing.txt\n",
      "desc_web_mobile_software_dev.txt\n",
      "desc_translation.txt\n",
      "desc_sales_and_marketing.txt\n",
      "desc_legal.txt\n",
      "desc_it_and_networking.txt\n",
      "desc_engineering_and_architecture.txt\n",
      "desc_design_and_creative.txt\n",
      "desc_data_science.txt\n",
      "desc_customer_service.txt\n",
      "desc_administrative_support.txt\n",
      "desc_accounting_and_consulting.txt\n",
      "title_writing.txt\n",
      "title_web_software_dev.txt\n",
      "title_translation.txt\n",
      "title_sales_and_marketing.txt\n",
      "title_legal.txt\n",
      "title_it_networking.txt\n",
      "title_engineering_and_architecture.txt\n",
      "title_design_and_creative.txt\n",
      "title_data_science.txt\n",
      "title_customer_service.txt\n",
      "title_admin_support.txt\n",
      "title_accounting_and_consulting.txt\n",
      "[nltk_data] Downloading collection u'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ecf384/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "######### MATCHING ON JOB KEYWORDS ##########\n",
      "writing |academic |writing |research |article |blog |writing |copywriting |creative |writing |editing |proofreading |grant |writing |resumes |cover |letters |technical |writing |web |content \n",
      " Market Research Analysts and Marketing Specialists\n",
      " Computer and Information Research Scientists\n",
      " Web Developers\n",
      " Operations Research Analysts\n",
      " Social Science Research Assistants\n",
      " Middle School Teachers, Except Special and Career/Technical Education\n",
      " Career/Technical  Education Teachers, Middle School\n",
      " Secondary School Teachers, Except Special and Career/Technical Education\n",
      " Career/Technical Education Teachers, Secondary School\n",
      " Technical Writers\n",
      " Healthcare Practitioners and Technical Workers, All Other\n",
      " Sales Representatives, Wholesale and Manufacturing, Technical and Scientific Products\n",
      " Sales Representatives, Wholesale and Manufacturing, Except Technical and Scientific Products\n",
      " Production, Planning, and Expediting Clerks\n",
      "######### MATCHING ON JOB CATEGORY ##########\n",
      "----MATCHES ON JOB DESCRIPTION----\n",
      " Coaches and Scouts\n",
      " Marketing Managers\n",
      " Hydrologists\n",
      "Team Assemblers\n",
      " Anthropologists and Archeologists\n",
      "----MATCHES ON JOB TITLES----\n",
      " Data Entry Keyers\n",
      " Door-to-Door Sales Workers, News and Street Vendors, and Related Workers\n",
      " Demonstrators and Product Promoters\n",
      "Supervisors of Construction and Extraction Workers\n",
      " Agents and Business Managers of Artists, Performers, and Athletes\n",
      "----MATCH FROM UPWORK TITLES TO SOC JOBS----\n",
      " Social Science Research Assistants\n",
      " Data Entry Keyers\n",
      " Cartographers and Photogrammetrists\n",
      " Computer Operators\n",
      "Mining Machine Operators, All Other\n",
      "----MATCH FROM UPWORK TITLES TO SOC TITLES----\n",
      " Data Entry Keyers\n",
      "Mining Machine Operators, All Other\n",
      "Extraction Workers, All Other\n",
      " Continuous Mining Machine Operators\n",
      " Helpers--Extraction Workers\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "directory = './job_descriptions'\n",
    "title_directory = './job_titles'\n",
    "upwork_descriptions = []\n",
    "upwork_titles = []\n",
    "no_job_categories = 12\n",
    "\n",
    "for f in os.listdir(directory):\n",
    "    print f\n",
    "    content = open(directory + '/' + f).read()\n",
    "    upwork_descriptions.append(content)\n",
    "\n",
    "for f in os.listdir(title_directory):\n",
    "    print f\n",
    "    content = open(title_directory + '/' + f).read()\n",
    "    upwork_titles.append(content)\n",
    "    \n",
    "# Extract descriptions from SOC job description list\n",
    "df = pd.read_csv('./soc_2010_definitions.csv')\n",
    "\n",
    "soc_descriptions = []\n",
    "soc_titles = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    soc_descriptions.append(row['SOC Definition'])\n",
    "    soc_titles.append(row['SOC Title'])\n",
    "\n",
    "complete_job_array = upwork_descriptions + soc_descriptions\n",
    "complete_title_array = upwork_descriptions + soc_titles\n",
    "title_job_array = upwork_titles + soc_descriptions\n",
    "title_title_array = upwork_titles + soc_titles\n",
    "\n",
    "tfidf = TfidfVectorizer().fit_transform(complete_job_array)\n",
    "tfidf_title = TfidfVectorizer().fit_transform(complete_title_array)\n",
    "tfidf_title_job = TfidfVectorizer().fit_transform(title_job_array)\n",
    "tfidf_title2 = TfidfVectorizer().fit_transform(title_title_array)\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "pairwise_similarity = (tfidf * tfidf.T).A\n",
    "pairwise_title_similarity = (tfidf_title * tfidf_title.T).A\n",
    "pairwise_title_job_similarity = (tfidf_title_job * tfidf_title_job.T).A\n",
    "pairwise_title2_similarity = (tfidf_title2 * tfidf_title2.T).A\n",
    "\n",
    "# Predict job category\n",
    "\n",
    "def return_job_category(soc_index): \n",
    "    print df.iloc[soc_index]['SOC Title']\n",
    "    \n",
    "def match_job_category(job_index):\n",
    "    max_jobs = 5 # Set number of job categories you want to return \n",
    "    max_indices = []\n",
    "    job_array = pairwise_similarity[job_index]\n",
    "    title_array = pairwise_title_similarity[job_index]\n",
    "    title_job_array = pairwise_title_job_similarity[job_index]\n",
    "    title2_array = pairwise_title2_similarity[job_index]\n",
    "    \n",
    "    # Find the highest TFIDF value among the job categories we are trying to match to\n",
    "    soc_jobs = job_array[no_job_categories:]\n",
    "    soc_titles = title_array[no_job_categories:]\n",
    "    soc_title_job = title_job_array[no_job_categories:]\n",
    "    soc_title2 = title2_array[no_job_categories:]\n",
    "    df['match_' + str(job_index)] = soc_jobs\n",
    "    df['title_match_' + str(job_index)] = soc_titles\n",
    "    df['title_job_match_' + str(job_index)] = soc_title_job\n",
    "    df['title2_match_' + str(job_index)] = soc_title2\n",
    "\n",
    "    print \"----MATCHES ON JOB DESCRIPTION----\"\n",
    "    for i in df.nlargest(5, 'match_' + str(job_index)).index:\n",
    "        print df[df.index == i]['SOC Title'].values[0]\n",
    "        #print df[df.index == i]['SOC Definition'].values[0]\n",
    "    \n",
    "    print \"----MATCHES ON JOB TITLES----\"\n",
    "    for i in df.nlargest(5, 'title_match_' + str(job_index)).index:\n",
    "        print df[df.index == i]['SOC Title'].values[0]\n",
    "        #print df[df.index == i]['SOC Definition'].values[0]\n",
    "        \n",
    "    print \"----MATCH FROM UPWORK TITLES TO SOC JOBS----\"\n",
    "    for i in df.nlargest(5, 'title_job_match_' + str(job_index)).index:\n",
    "        print df[df.index == i]['SOC Title'].values[0]\n",
    "        #print df[df.index == i]['SOC Definition'].values[0]\n",
    "    \n",
    "    print \"----MATCH FROM UPWORK TITLES TO SOC TITLES----\"\n",
    "    for i in df.nlargest(5, 'title2_match_' + str(job_index)).index:\n",
    "        print df[df.index == i]['SOC Title'].values[0]\n",
    "        #print df[df.index == i]['SOC Definition'].values[0]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def match_on_keywords(job_index, soc_dataset):\n",
    "    \n",
    "    title_keywords = upwork_titles[job_index]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(title_keywords)\n",
    "    #filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            if w != '&':\n",
    "                filtered_sentence.append(w)\n",
    "    \n",
    "    pattern = \"\"\n",
    "    for keyword in filtered_sentence:\n",
    "        pattern += keyword.lower() + \" |\"\n",
    "    \n",
    "    pattern = pattern[:len(pattern)-1]\n",
    "    print pattern\n",
    "        \n",
    "    for job in soc_dataset:\n",
    "        if re.search(pattern, job.lower()):\n",
    "            print job\n",
    "\n",
    "nltk.download('popular')\n",
    "print \"######### MATCHING ON JOB KEYWORDS ##########\"\n",
    "match_on_keywords(0, soc_titles)\n",
    "\n",
    "print \"######### MATCHING ON JOB CATEGORY ##########\"\n",
    "match_job_category(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-41913f7afef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'dog cat banana'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hello world\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
